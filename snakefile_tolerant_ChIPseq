# Snakefile for ChIP-seq analysis pipeline

import pandas as pd
import os

# Configuration
configfile: "ref/config.yaml"

# Load sample information from a table
samples_df = pd.read_csv(config["samples_table"])

# Extract sample information
SAMPLES = samples_df['sample_id'].tolist()

# Define output directories
RESULT_DIR = "results"
FASTQC_DIR = f"{RESULT_DIR}/fastqc"
FASTP_DIR = f"{RESULT_DIR}/fastp"
ALIGN_DIR = f"{RESULT_DIR}/aligned"
TMP_DIR = f"{RESULT_DIR}/tmp"
FILTERED_DIR = f"{RESULT_DIR}/filtered"
DEDUP_DIR = f"{RESULT_DIR}/dedup"
BLACKLIST_FILTERED_DIR = f"{RESULT_DIR}/blacklist_filtered"
PEAKS_DIR = f"{RESULT_DIR}/peaks"
QC_DIR = f"{RESULT_DIR}/qc"
BIGWIG_DIR = f"{RESULT_DIR}/bigwig"
NORMALIZED_BIGWIG_DIR = f"{RESULT_DIR}/normalized_bigwig"


# Specify that certain rules should be run locally
localrules: fastqc, process_all_bams
# Define rule execution order explicitly
ruleorder: fastp > hisat2_align > samtools_sort_filter_index > remove_duplicates > filter_blacklist > call_narrow_peaks > call_broad_peaks

# Precompute the peak types and input controls
BROAD_SAMPLES = []
NARROW_SAMPLES = []
INPUT_CONTROLS = {}

for sample in SAMPLES:
    # Determine if this sample uses broad peak calling
    is_broad = False
    if 'peak_mode' in samples_df.columns:
        sample_info = samples_df.loc[samples_df['sample_id'] == sample]
        if not sample_info.empty and not pd.isna(sample_info['peak_mode'].iloc[0]):
            mode = sample_info['peak_mode'].iloc[0].lower()
            if mode == 'broad':
                is_broad = True
    
    # Append to the appropriate list
    if is_broad:
        BROAD_SAMPLES.append(sample)
    else:
        NARROW_SAMPLES.append(sample)
    
    # Determine if this sample has an input control
    if 'input_control' in samples_df.columns:
        sample_info = samples_df.loc[samples_df['sample_id'] == sample]
        if not sample_info.empty and not pd.isna(sample_info['input_control'].iloc[0]):
            input_control = sample_info['input_control'].iloc[0]
            INPUT_CONTROLS[sample] = input_control

# Define final output files
rule all:
    input:
        # FastQC reports
        expand(f"{FASTQC_DIR}/{{sample}}_R1_001_fastqc.html", sample=SAMPLES),
        expand(f"{FASTQC_DIR}/{{sample}}_R2_001_fastqc.html", sample=SAMPLES),
        
        # Fastp reports and trimmed reads
        expand(f"{FASTP_DIR}/{{sample}}.html", sample=SAMPLES),
        expand(f"{FASTP_DIR}/{{sample}}.json", sample=SAMPLES),
        
        # Aligned SAM files
        expand(f"{ALIGN_DIR}/{{sample}}.sam", sample=SAMPLES),
        expand(f"{ALIGN_DIR}/{{sample}}.sam.summary", sample=SAMPLES),
        
        # Filtered BAM files
        expand(f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam", sample=SAMPLES),
        expand(f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam.bai", sample=SAMPLES),
        
        # Deduplicated BAM files
        expand(f"{DEDUP_DIR}/{{sample}}.dedup.bam", sample=SAMPLES),
        expand(f"{DEDUP_DIR}/{{sample}}.dedup.metrics.txt", sample=SAMPLES),
        
        # Blacklist filtered BAM files
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam", sample=SAMPLES),
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai", sample=SAMPLES),
        
        # Peak calling results (with blacklist filtering)
        expand(f"{PEAKS_DIR}/{{sample}}_peaks.narrowPeak", sample=NARROW_SAMPLES),
        expand(f"{PEAKS_DIR}/{{sample}}_peaks.broadPeak", sample=BROAD_SAMPLES),
        
        # Blacklist filtering statistics
        f"{QC_DIR}/blacklist_filtering_stats.txt",
        
        # Normalized
        # bigwig files
        expand(f"{BIGWIG_DIR}/{{sample}}.bw", sample=SAMPLES),
        expand(f"{NORMALIZED_BIGWIG_DIR}/{{sample}}.normalized.bw", sample=[s for s in SAMPLES if s in INPUT_CONTROLS])

# FastQC on raw reads
rule fastqc:
    priority: 1  # Higher priority value makes it run later
    input:
        r1 = "data/{sample}_R1_001.fastq.gz",
        r2 = "data/{sample}_R2_001.fastq.gz"
    output:
        r1_html = f"{FASTQC_DIR}/{{sample}}_R1_001_fastqc.html",
        r2_html = f"{FASTQC_DIR}/{{sample}}_R2_001_fastqc.html"
    params:
        outdir = FASTQC_DIR
    threads: 4
    log:
        "logs/fastqc/{sample}.log"
    shell:
        """
        mkdir -p {params.outdir}
        fastqc -t {threads} -o {params.outdir} {input.r1} {input.r2} > {log} 2>&1
        """

# Fastp for read trimming and quality filtering
rule fastp:
    priority: 2  # Higher priority value makes it run later
    input:
        r1 = "data/{sample}_R1_001.fastq.gz",
        r2 = "data/{sample}_R2_001.fastq.gz"
    output:
        r1 = f"{FASTP_DIR}/{{sample}}_R1.trimmed.fastq.gz",
        r2 = f"{FASTP_DIR}/{{sample}}_R2.trimmed.fastq.gz",
        html = f"{FASTP_DIR}/{{sample}}.html",
        json = f"{FASTP_DIR}/{{sample}}.json"
    threads: 4
    log:
        "logs/fastp/{sample}.log"
    shell:
        """
        mkdir -p {FASTP_DIR}
        fastp --in1 {input.r1} --in2 {input.r2} \
              --out1 {output.r1} --out2 {output.r2} \
              --thread {threads} \
              --html {output.html} \
              --json {output.json} \
              --detect_adapter_for_pe --trim_poly_g\
              --length_required 30 -p --cut_front --cut_tail --cut_window_size 4 --cut_mean_quality 20 > {log} 2>&1
        """

# Align reads with HISAT2 (from ATAC-seq pipeline) instead of Bowtie2
rule hisat2_align:
    priority: 3  # Higher priority value makes it run later
    input:
        r1 = f"{FASTP_DIR}/{{sample}}_R1.trimmed.fastq.gz",
        r2 = f"{FASTP_DIR}/{{sample}}_R2.trimmed.fastq.gz"
    output:
        sam = f"{ALIGN_DIR}/{{sample}}.sam",
        summary = f"{ALIGN_DIR}/{{sample}}.sam.summary"
    log:
        "logs/hisat2/{sample}.log"
    params:
        index = config["hisat2_index"]  # Make sure to update config.yaml with hisat2_index
    threads: 20
    shell:
        """
        mkdir -p {ALIGN_DIR} logs/hisat2
        hisat2 -x {params.index} -1 {input.r1} -2 {input.r2} \
               -S {output.sam} \
               --summary-file {output.summary} \
               --no-spliced-alignment --add-chrname \
               -p {threads} \
               -q --phred33 -X 3000 -I 0 --no-discordant --no-mixed \
               --score-min L,0,-0.6 \
               --mp 4,2 \
               --rdg 5,3 --rfg 5,3 \
               &> {log}
        """

# Filter, sort, index BAM and generate flagstat (from ATAC-seq pipeline)
rule samtools_sort_filter_index:
    priority: 5  # Higher priority value makes it run later
    input:
        f"{ALIGN_DIR}/{{sample}}.sam"
    output:
        bam = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam",
        bai = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam.bai",
        flagstat = f"{FILTERED_DIR}/{{sample}}_summary.txt"
    log:
        "logs/samtools/{sample}.log"
    threads: 20
    shell:
        """
        mkdir -p {FILTERED_DIR} logs/samtools {TMP_DIR}
        samtools view -@ {threads} -f 0x2 -F 0x100 -hS {input} | grep "NH:i:1\\|^@" > {TMP_DIR}/tmp_{wildcards.sample}.sam
        samtools view -@ {threads} -bhS {TMP_DIR}/tmp_{wildcards.sample}.sam | \
        samtools sort -@ {threads} -O bam -o {output.bam}
        samtools index -@ {threads} {output.bam} {output.bai}
        samtools flagstat {output.bam} > {output.flagstat}
        rm {TMP_DIR}/tmp_{wildcards.sample}.sam
        """

# Remove duplicates with Picard
rule remove_duplicates:
    priority: 9  # Higher priority value makes it run later
    input:
        filtered_bam = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam"
    output:
        dedup_bam = f"{DEDUP_DIR}/{{sample}}.dedup.bam",
        metrics = f"{DEDUP_DIR}/{{sample}}.dedup.metrics.txt"
    threads: 4
    log:
        "logs/dedup/{sample}.log"
    shell:
        """
        mkdir -p {DEDUP_DIR}
        java -jar ref/picard.jar MarkDuplicates \
               INPUT={input.filtered_bam} \
               OUTPUT={output.dedup_bam} \
               METRICS_FILE={output.metrics} \
               REMOVE_DUPLICATES=true \
               ASSUME_SORTED=true \
               VALIDATION_STRINGENCY=LENIENT \
               TMP_DIR=tmp 2> {log}
        samtools index {output.dedup_bam}
        """

# Filter against blacklist regions
rule filter_blacklist:
    priority: 10
    input:
        bam = f"{DEDUP_DIR}/{{sample}}.dedup.bam",
        blacklist = config["blacklist"]
    output:
        filtered_bam = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam",
        filtered_bai = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai",
        excluded_reads = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.blacklisted.bam"
    params:
        temp_bedpe = f"{TMP_DIR}/{{sample}}.fragments.bedpe",
        temp_fragment_bed = f"{TMP_DIR}/{{sample}}.fragments.bed",
        temp_blacklist_fragments = f"{TMP_DIR}/{{sample}}.blacklisted.fragments.bed",
        temp_blacklist_ids = f"{TMP_DIR}/{{sample}}.blacklisted.ids.txt",
        temp_namesorted_bam = f"{TMP_DIR}/{{sample}}.namesorted.bam",
        temp_filtered_bam = f"{TMP_DIR}/{{sample}}.filtered.bam",
        temp_excluded_bam = f"{TMP_DIR}/{{sample}}.excluded.bam"
    threads: 8
    log:
        "logs/blacklist_filter/{sample}.log"
    shell:
        """
        mkdir -p {BLACKLIST_FILTERED_DIR} {TMP_DIR}
        
        # Convert BAM to BEDPE format
        # This creates a BEDPE file with paired-end information
        samtools sort -n -@ {threads} {input.bam} | \
        bedtools bamtobed -bedpe -i stdin > {params.temp_bedpe} 2> {log}
        
        # Convert BEDPE to fragment BED (one entry per fragment)
        # Format: chrom, fragment_start, fragment_end, read_name, score, strand
        # We'll extract the fragment coordinates (minimum start, maximum end) 
        # and keep the read name for later filtering
        awk 'BEGIN {{OFS="\\t"}} {{if ($1==$4) print $1, ($2<$5?$2:$5), ($3>$6?$3:$6), $7, ".", ($9=="+"?"+":"-")}}' \
        {params.temp_bedpe} > {params.temp_fragment_bed} 2>> {log}
        
        # Find fragments that intersect with blacklisted regions
        bedtools intersect -a {params.temp_fragment_bed} -b {input.blacklist} -wa > {params.temp_blacklist_fragments} 2>> {log}
        
        # Extract read IDs from blacklisted fragments
        cut -f4 {params.temp_blacklist_fragments} | sort | uniq > {params.temp_blacklist_ids} 2>> {log}
        
        # Sort BAM by read name for paired-end processing (if not already name-sorted)
        samtools sort -n -@ {threads} -o {params.temp_namesorted_bam} {input.bam} 2>> {log}
        
        # Create properly paired BAMs - one with fragments that don't overlap blacklist
        
        # Filter out fragments overlapping blacklisted regions
        samtools view -@ {threads} -b -N ^{params.temp_blacklist_ids} \
            {params.temp_namesorted_bam} > {params.temp_filtered_bam} 2>> {log}
            
        # Extract fragments overlapping blacklisted regions
        samtools view -@ {threads} -b -N {params.temp_blacklist_ids} \
            {params.temp_namesorted_bam} > {params.temp_excluded_bam} 2>> {log}
            
        # Sort filtered BAM (non-blacklisted fragments) by coordinate for final output
        samtools sort -@ {threads} -o {output.filtered_bam} {params.temp_filtered_bam} 2>> {log}
        
        # Sort excluded BAM (blacklisted fragments) by coordinate for QC
        samtools sort -@ {threads} -o {output.excluded_reads} {params.temp_excluded_bam} 2>> {log}
        
        # Index the filtered BAM
        samtools index -@ {threads} {output.filtered_bam} {output.filtered_bai} 2>> {log}
        
        # Clean up temporary files
        rm -f {params.temp_bedpe} {params.temp_fragment_bed} {params.temp_blacklist_fragments} \
            {params.temp_blacklist_ids} {params.temp_namesorted_bam} {params.temp_filtered_bam} \
            {params.temp_excluded_bam}
        
        # Report stats
        echo "Blacklist filtering completed for {wildcards.sample}" >> {log}
        echo "$(wc -l < {params.temp_blacklist_fragments}) fragments overlap blacklisted regions" >> {log}
        echo "$(wc -l < {params.temp_blacklist_ids}) unique fragment IDs overlapping blacklisted regions" >> {log}
        echo "$(samtools view -c {output.excluded_reads}) total reads in excluded fragments" >> {log}
        echo "$(samtools view -c {output.filtered_bam}) total reads in filtered output" >> {log}
        """

rule process_all_bams:
    input:
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam", sample=SAMPLES),
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai", sample=SAMPLES)
    output:
        touch("results/bams_processed.flag")

# Call narrow peaks with MACS2
rule call_narrow_peaks:
    priority: 100  # Higher priority value makes it run later
    input:
        treatment = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam",
        checkpoint = "results/bams_processed.flag"  # Add this line
    output:
        peaks = f"{PEAKS_DIR}/{{sample}}_peaks.narrowPeak"
    params:
        outdir = PEAKS_DIR,
        name = "{sample}",
        # Check if this sample has an input control and construct the control parameter
        control = lambda wildcards: f"{BLACKLIST_FILTERED_DIR}/{INPUT_CONTROLS[wildcards.sample]}.nobl.bam" if wildcards.sample in INPUT_CONTROLS else "",
        has_control = lambda wildcards: "yes" if wildcards.sample in INPUT_CONTROLS else "no"
    conda:
        "envs/macs2.yaml"
    log:
        "logs/macs2/{sample}_narrow.log"
    shell:
        """
        mkdir -p {params.outdir}
        
        # Check if we have an input control
        if [ "{params.has_control}" == "yes" ]; then
            # Run MACS2 with input control
            macs2 callpeak \
                -t {input.treatment} \
                -c {params.control} \
                --format BAMPE \
                -g hs \
                --outdir {params.outdir} \
                -n {params.name} \
                --keep-dup all \
                -q 0.05 > {log} 2>&1
        else
            # Run MACS2 without input control
            macs2 callpeak \
                -t {input.treatment} \
                -f BAMPE \
                -g hs \
                --outdir {params.outdir} \
                -n {params.name} \
                --nomodel \
                --shift -75 \
                --extsize 150 \
                -q 0.05 > {log} 2>&1
        fi
        """

# Call broad peaks with MACS2
rule call_broad_peaks:
    priority: 100  # Higher priority value makes it run later
    input:
        treatment = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam",
        checkpoint = "results/bams_processed.flag"  # Add this line
    output:
        peaks = f"{PEAKS_DIR}/{{sample}}_peaks.broadPeak"
    params:
        outdir = PEAKS_DIR,
        name = "{sample}",
        # Check if this sample has an input control and construct the control parameter
        control = lambda wildcards: f"{BLACKLIST_FILTERED_DIR}/{INPUT_CONTROLS[wildcards.sample]}.nobl.bam" if wildcards.sample in INPUT_CONTROLS else "",
        has_control = lambda wildcards: "yes" if wildcards.sample in INPUT_CONTROLS else "no"
    conda:
        "envs/macs2.yaml"
    log:
        "logs/macs2/{sample}_broad.log"
    shell:
        """
        mkdir -p {params.outdir}
        
        # Check if we have an input control
        if [ "{params.has_control}" == "yes" ]; then
            # Run MACS2 with input control
            macs2 callpeak \
                -t {input.treatment} \
                -c {params.control} \
                --format BAMPE \
                -g hs \
                --outdir {params.outdir} \
                -n {params.name} \
                --broad \
                --keep-dup all \
                -q 0.05 > {log} 2>&1
        else
            # Run MACS2 without input control
            macs2 callpeak \
                -t {input.treatment} \
                -f BAMPE \
                -g hs \
                --outdir {params.outdir} \
                -n {params.name} \
                --broad \
                --nomodel \
                --shift -75 \
                --extsize 150 \
                -q 0.05 > {log} 2>&1
        fi
        """

# Generate blacklist filtering statistics
rule blacklist_stats:
    input:
        original_bams = expand(f"{DEDUP_DIR}/{{sample}}.dedup.bam", sample=SAMPLES),
        filtered_bams = expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam", sample=SAMPLES),
        excluded_bams = expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.blacklisted.bam", sample=SAMPLES)
    output:
        stats = f"{QC_DIR}/blacklist_filtering_stats.txt"
    params:
        samples = SAMPLES  # Pass sample names to the script
    threads: 1
    log:
        "logs/blacklist_stats/summary.log"
    script:
        "ref/blacklist-stats-script.py"

# Create bigwig files for all samples (both ChIP and input)
rule create_bigwig:
    priority: 130
    input:
        bam = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam",
        bai = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai"
    output:
        bigwig = f"{BIGWIG_DIR}/{{sample}}.bw"
    params:
        effective_genome_size = config["effective_genome_size"],
        bin_size = 25,
        blacklist = config["blacklist"]
    threads: 8
    conda:
        "envs/deeptools.yaml"
    log:
        "logs/bigwig/{sample}.log"
    shell:
        """
        mkdir -p {BIGWIG_DIR} logs/bigwig
        
        bamCoverage --bam {input.bam} \
            --normalizeUsing RPGC \
            --effectiveGenomeSize {params.effective_genome_size} \
            --binSize {params.bin_size} \
            --numberOfProcessors {threads} \
            --outFileName {output.bigwig} \
            --outFileFormat bigwig \
            --extendReads \
            --blackListFileName {params.blacklist} > {log} 2>&1
        """

rule create_normalized_bigwig:
    priority: 131
    input:
        signal_bw = f"{BIGWIG_DIR}/{{sample}}.bw",
        input_bw = lambda wildcards: f"{BIGWIG_DIR}/{INPUT_CONTROLS[wildcards.sample]}.bw" if wildcards.sample in INPUT_CONTROLS else []
    output:
        normalized_bw = f"{NORMALIZED_BIGWIG_DIR}/{{sample}}.normalized.bw"
    params:
        bin_size = 25,
        pseudocount = 1
    threads: 8
    conda:
        "envs/deeptools.yaml"
    log:
        "logs/normalized_bigwig/{sample}.log"
    shell:
        """
        mkdir -p {NORMALIZED_BIGWIG_DIR} logs/normalized_bigwig
        
        bigwigCompare -b1 {input.signal_bw} \
            -b2 {input.input_bw} \
            --operation ratio \
            --binSize {params.bin_size} \
            --numberOfProcessors {threads} \
            --pseudocount {params.pseudocount} \
            -o {output.normalized_bw} > {log} 2>&1
        """